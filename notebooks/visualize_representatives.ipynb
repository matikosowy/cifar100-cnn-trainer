{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "from cifar100cnn.data import get_cifar_data\n",
    "from cifar100cnn.extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATHS = {\n",
    "    \"resnet18_scratch\": \"../checkpoints/resnet18/from-scratch/best_model.pth\",\n",
    "    \"resnet50_scratch\": \"../checkpoints/resnet50/from-scratch/best_model.pth\",\n",
    "    \"resnet18_fine_tuned\": \"../checkpoints/resnet18/fine-tuned/best_model.pth\",\n",
    "    \"resnet50_fine_tuned\": \"../checkpoints/resnet50/fine-tuned/best_model.pth\",\n",
    "    \"wide_resnet\": \"../checkpoints/wide_resnet28_10/best_model.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, path, device):\n",
    "    sys.path.append('..')\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Plik {path} nie istnieje\")\n",
    "\n",
    "    if \"resnet18\" in model_name:\n",
    "        model = ResNet(version=18, num_classes=50, pretrained=False)\n",
    "    elif \"resnet50\" in model_name:\n",
    "        model = ResNet(version=50, num_classes=50, pretrained=False)\n",
    "    elif \"wide_resnet\" in model_name:\n",
    "        model = WideResNet(depth=28, widen_factor=10, dropout_rate=0.5, num_classes=50)\n",
    "    else:\n",
    "        raise ValueError(f\"Nieznany model: {model_name}\")\n",
    "\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Saved class names to class_names.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'apple',\n",
       " 1: 'aquarium_fish',\n",
       " 2: 'baby',\n",
       " 3: 'bear',\n",
       " 4: 'beaver',\n",
       " 5: 'bed',\n",
       " 6: 'bee',\n",
       " 7: 'beetle',\n",
       " 8: 'bicycle',\n",
       " 9: 'bottle',\n",
       " 10: 'bowl',\n",
       " 11: 'boy',\n",
       " 12: 'bridge',\n",
       " 13: 'bus',\n",
       " 14: 'butterfly',\n",
       " 15: 'camel',\n",
       " 16: 'can',\n",
       " 17: 'castle',\n",
       " 18: 'caterpillar',\n",
       " 19: 'cattle',\n",
       " 20: 'chair',\n",
       " 21: 'chimpanzee',\n",
       " 22: 'clock',\n",
       " 23: 'cloud',\n",
       " 24: 'cockroach',\n",
       " 25: 'couch',\n",
       " 26: 'crab',\n",
       " 27: 'crocodile',\n",
       " 28: 'cup',\n",
       " 29: 'dinosaur',\n",
       " 30: 'dolphin',\n",
       " 31: 'elephant',\n",
       " 32: 'flatfish',\n",
       " 33: 'forest',\n",
       " 34: 'fox',\n",
       " 35: 'girl',\n",
       " 36: 'hamster',\n",
       " 37: 'house',\n",
       " 38: 'kangaroo',\n",
       " 39: 'keyboard',\n",
       " 40: 'lamp',\n",
       " 41: 'lawn_mower',\n",
       " 42: 'leopard',\n",
       " 43: 'lion',\n",
       " 44: 'lizard',\n",
       " 45: 'lobster',\n",
       " 46: 'man',\n",
       " 47: 'maple_tree',\n",
       " 48: 'motorcycle',\n",
       " 49: 'mountain',\n",
       " 50: 'mouse',\n",
       " 51: 'mushroom',\n",
       " 52: 'oak_tree',\n",
       " 53: 'orange',\n",
       " 54: 'orchid',\n",
       " 55: 'otter',\n",
       " 56: 'palm_tree',\n",
       " 57: 'pear',\n",
       " 58: 'pickup_truck',\n",
       " 59: 'pine_tree',\n",
       " 60: 'plain',\n",
       " 61: 'plate',\n",
       " 62: 'poppy',\n",
       " 63: 'porcupine',\n",
       " 64: 'possum',\n",
       " 65: 'rabbit',\n",
       " 66: 'raccoon',\n",
       " 67: 'ray',\n",
       " 68: 'road',\n",
       " 69: 'rocket',\n",
       " 70: 'rose',\n",
       " 71: 'sea',\n",
       " 72: 'seal',\n",
       " 73: 'shark',\n",
       " 74: 'shrew',\n",
       " 75: 'skunk',\n",
       " 76: 'skyscraper',\n",
       " 77: 'snail',\n",
       " 78: 'snake',\n",
       " 79: 'spider',\n",
       " 80: 'squirrel',\n",
       " 81: 'streetcar',\n",
       " 82: 'sunflower',\n",
       " 83: 'sweet_pepper',\n",
       " 84: 'table',\n",
       " 85: 'tank',\n",
       " 86: 'telephone',\n",
       " 87: 'television',\n",
       " 88: 'tiger',\n",
       " 89: 'tractor',\n",
       " 90: 'train',\n",
       " 91: 'trout',\n",
       " 92: 'tulip',\n",
       " 93: 'turtle',\n",
       " 94: 'wardrobe',\n",
       " 95: 'whale',\n",
       " 96: 'willow_tree',\n",
       " 97: 'wolf',\n",
       " 98: 'woman',\n",
       " 99: 'worm'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, class_names = get_cifar_data(num_classes=100, augment=True)\n",
    "class_dict = {idx: name for idx, name in enumerate(class_names)}\n",
    "\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jedno\\AppData\\Local\\Temp\\ipykernel_14348\\3110914763.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "classes_A = np.arange(50)\n",
    "classes_B = np.arange(50, 100)\n",
    "\n",
    "models = {name: load_model(name, path, device) for name, path in MODEL_PATHS.items()}\n",
    "feature_extractor = FeatureExtractor(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, representative, test_loader, class_type):\n",
    "    test_features, test_labels = feature_extractor.extract_features(model, test_loader)\n",
    "\n",
    "    mask_test = np.isin(test_labels, class_type)\n",
    "    test_features, test_labels_A = test_features[mask_test], test_labels[mask_test]\n",
    "\n",
    "    predictions = classify_knn(test_features, representative)\n",
    "    accuracy = evaluate_accuracy(predictions, test_labels_A)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Dodaj .item()!\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Odtworzenie struktury słownika\u001b[39;00m\n\u001b[0;32m     65\u001b[0m class_representatives \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(k): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loaded_data\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "def plot_stages_side_by_side(pca_results_A, pca_results_B, classes_list_A, classes_list_B, \n",
    "                            accuracies_A, accuracies_B, model_name, class_dict):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    sample_counts = [1, 5, 10]\n",
    "\n",
    "    # Etap A (dane treningowe)\n",
    "    for col, num_samples in enumerate(sample_counts):\n",
    "        idx = sample_counts.index(num_samples)\n",
    "        features_pca = pca_results_A[idx]\n",
    "        classes = classes_list_A[idx]\n",
    "        accuracy = accuracies_A[idx]\n",
    "        \n",
    "        scatter = axs[0, col].scatter(features_pca[:, 0], features_pca[:, 1], c=classes, cmap='tab20', s=100, alpha=0.8)\n",
    "        axs[0, col].set_title(f\"Etap A ({num_samples} próbek/klasę)\\nAccuracy: {accuracy:.2f}% (kNN cosine)\")\n",
    "        axs[0, col].set_xticks([])\n",
    "        axs[0, col].set_yticks([])\n",
    "\n",
    "        # Dodawanie nazw klas do wykresu\n",
    "        for i, txt in enumerate(classes):\n",
    "            axs[0, col].text(features_pca[i, 0], features_pca[i, 1], class_dict[txt], fontsize=9)\n",
    "\n",
    "    # Etap B (dane testowe)\n",
    "    for col, num_samples in enumerate(sample_counts):\n",
    "        idx = sample_counts.index(num_samples)\n",
    "        features_pca = pca_results_B[idx]\n",
    "        classes = classes_list_B[idx]\n",
    "        accuracy = accuracies_B[idx]\n",
    "        \n",
    "        scatter = axs[1, col].scatter(features_pca[:, 0], features_pca[:, 1], c=classes, cmap='tab20', s=100, alpha=0.8)\n",
    "        axs[1, col].set_title(f\"Etap B ({num_samples} próbek/klasę)\\nAccuracy: {accuracy:.2f}% (kNN cosine)\")\n",
    "        axs[1, col].set_xticks([])\n",
    "        axs[1, col].set_yticks([])\n",
    "\n",
    "        # Dodawanie nazw klas do wykresu\n",
    "        for i, txt in enumerate(classes):\n",
    "            axs[1, col].text(features_pca[i, 0], features_pca[i, 1], class_dict[txt], fontsize=9)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.suptitle(f\"Model: {model_name}\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Ścieżki do folderów z danymi\n",
    "base_dir_A = os.path.join(\"..\", \"etap2\")\n",
    "base_dir_B = os.path.join(\"..\", \"etap3\")\n",
    "\n",
    "models_names = [\"resnet18_scratch\", \"resnet18_fine_tuned\", \"resnet50_scratch\", \"resnet50_fine_tuned\", \"wide_resnet\"]\n",
    "nums_samples = [1, 5, 10]\n",
    "\n",
    "for model_name in models_names:\n",
    "    # Inicjalizacja struktur danych\n",
    "    pca_results_A, pca_results_B = [], []\n",
    "    classes_list_A, classes_list_B = [], []\n",
    "    accuracies_A, accuracies_B = [], []\n",
    "\n",
    "    # Przetwarzanie Etapu A\n",
    "    for num_samples in nums_samples:\n",
    "        file_path = os.path.join(base_dir_A, f\"{model_name}_{num_samples}_rep.npy\")\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        loaded_data = np.load(file_path, allow_pickle=True).item()  # Dodaj .item()!\n",
    "\n",
    "        # Odtworzenie struktury słownika\n",
    "        class_representatives = {int(k): v for k, v in loaded_data.items()}\n",
    "\n",
    "        classes = list(class_representatives.keys())\n",
    "        features = np.array(list(class_representatives.values()))\n",
    "\n",
    "\n",
    "\n",
    "        # Redukcja wymiarowości za pomocą PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        \n",
    "        # Dodanie wyników do list\n",
    "        pca_results_A.append(features_pca)\n",
    "        classes_list_A.append(classes)\n",
    "        \n",
    "        # Obliczanie accuracy dla Etapu A\n",
    "        accuracy_A = calculate_accuracy(models[model_name], class_representatives, test_loader, classes_A)\n",
    "        accuracies_A.append(accuracy_A)\n",
    "\n",
    "    # Przetwarzanie Etapu B\n",
    "    for num_samples in nums_samples:\n",
    "        file_path = os.path.join(base_dir_B, f\"{model_name}_{num_samples}.npy\")\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        loaded_data = np.load(file_path, allow_pickle=True).item()  # Dodaj .item()!\n",
    "\n",
    "        # Odtworzenie struktury słownika\n",
    "        class_representatives = {int(k): v for k, v in loaded_data.items()}\n",
    "\n",
    "        classes = list(class_representatives.keys())\n",
    "        features = np.array(list(class_representatives.values()))\n",
    "\n",
    "\n",
    "        # Redukcja wymiarowości za pomocą PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        \n",
    "        # Dodanie wyników do list\n",
    "        pca_results_B.append(features_pca)\n",
    "        classes_list_B.append(classes)\n",
    "        \n",
    "        # Obliczanie accuracy dla Etapu B\n",
    "        accuracy_B = calculate_accuracy(models[model_name], class_representatives, test_loader, classes_B)\n",
    "        accuracies_B.append(accuracy_B)\n",
    "\n",
    "    # Generowanie wykresów\n",
    "    if pca_results_A and pca_results_B:\n",
    "        plot_stages_side_by_side(pca_results_A, pca_results_B, \n",
    "                                classes_list_A, classes_list_B, \n",
    "                                accuracies_A, accuracies_B, \n",
    "                                model_name, class_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
